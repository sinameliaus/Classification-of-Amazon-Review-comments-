{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "inputData=pd.read_excel('BOYCE_Data_Preperation.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to check the balance of models\n",
    "#Returns number and percentage of each Label\n",
    "\n",
    "#Total_Reads\n",
    "def check_balance_reads(dataframe):\n",
    "    print('Number of 0 reads: ', len(dataframe[dataframe['Total_Reads'] == 0]))\n",
    "    print('Number of greater-than 0 reads: ', len(dataframe[dataframe['Total_Reads'] > 0]))\n",
    "    print('The ratio of the dataset is (zero / greater-than zero): %.2f%% / %.2f%%' % \n",
    "          ((round((len(dataframe[dataframe['Total_Reads'] == 0]) / len(dataframe)) * 100 , 2)), \n",
    "          (round((len(dataframe[dataframe['Total_Reads'] > 0]) / len(dataframe)) * 100 , 2))))\n",
    "    print(\"\")\n",
    "\n",
    "#Helpfulness_Label\n",
    "def check_balance_help(dataframe):\n",
    "    print('Number of Helpful: ', len(dataframe[dataframe['Helpfulness_Label'] == 0]))\n",
    "    print('Number of Unhelpful: ', len(dataframe[dataframe['Helpfulness_Label'] ==1]))\n",
    "    print('The ratio of the dataset is (helpful / unhelpful): %.2f%% / %.2f%%' %\n",
    "          ((round((len(dataframe[dataframe['Helpfulness_Label'] == 0]) / len(dataframe)) * 100 , 2)),\n",
    "           (round((len(dataframe[dataframe['Helpfulness_Label'] ==1]) / len(dataframe)) * 100 , 2))))\n",
    "    print(\"\")\n",
    "\n",
    "#Function to rebalance and slpit inital dataset (Class/Pred)\n",
    "def set_balance(inputData, title):\n",
    "    ##Prepare data for Classification & Prediction models\n",
    "    #Remove other Target attribute for each model\n",
    "    classData = inputData.drop([\"Total_Reads\"], axis = 1)\n",
    "    predData = inputData.drop([\"Helpfulness_Label\"], axis = 1)\n",
    "    \n",
    "    ##Classification data\n",
    "    #Check initial balance\n",
    "    print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"Helpfulness_Label Balance Check for: \"+ title + '\\033[0m')\n",
    "    print('\\033[1m' + \"Intitial: \"+ '\\033[0m')\n",
    "    check_balance_help(classData)\n",
    "    \n",
    "    ##Rebalance classification data\n",
    "    #Split dataframes\n",
    "    helpfulData = classData[classData['Helpfulness_Label'] == 0]\n",
    "    unhelpfulData = classData[classData['Helpfulness_Label'] == 1]\n",
    "\n",
    "    #Get smaller UNHELPFUL sample\n",
    "    desired_HELPFUL_ratio = 0.2   #(0.2/0.8 etc...)\n",
    "    newFrac = (((len(classData[classData['Helpfulness_Label'] == 0]) / desired_HELPFUL_ratio) \n",
    "               - len(classData[classData['Helpfulness_Label'] == 0])) / \n",
    "               len(classData[classData['Helpfulness_Label'] == 1]))\n",
    "    \n",
    "    #Rebalance\n",
    "    unhelpfulData = unhelpfulData.sample(frac=newFrac, random_state=1)\n",
    "    \n",
    "    #Rejoin\n",
    "    data_balanced = pd.concat([helpfulData, unhelpfulData])\n",
    "\n",
    "    #Shuffle dataframe and reset indexs to match number of rows\n",
    "    classData = data_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    #Check new balance\n",
    "    print('\\033[1m' + \"Rebalanced: \"+ '\\033[0m')\n",
    "    check_balance_help(classData)\n",
    "    \n",
    "    \n",
    "    ##Prediction data\n",
    "    #Check initial balance\n",
    "    print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"Total_Reads Balance Check for: \"+ title + '\\033[0m')\n",
    "    print('\\033[1m' + \"Intitial: \"+ '\\033[0m')\n",
    "    check_balance_reads(predData)\n",
    "    \n",
    "    ##Rebalancing prediction data\n",
    "    #Split dataframes\n",
    "    zeroReads = predData[predData['Total_Reads'] == 0]\n",
    "    largerReads = predData[predData['Total_Reads'] > 0]\n",
    "    \n",
    "    #Calculate percent to increase by (etc.. if zeroReads/nonZeroReads > 1)\n",
    "    if (len(predData[predData['Total_Reads'] == 0]) / len(predData[predData['Total_Reads'] > 0]) >= 1):\n",
    "        newFrac = len(predData[predData['Total_Reads'] == 0]) / len(predData[predData['Total_Reads'] > 0])\n",
    "        largerReads = largerReads.sample(frac=newFrac, random_state=1, replace=True)\n",
    "        predData = pd.concat([largerReads, zeroReads])\n",
    "    else:\n",
    "        predData = pd.concat([largerReads, zeroReads])\n",
    "\n",
    "    #Shuffle dataframe and reset indexs to match number of rows\n",
    "    predData = predData.sample(frac=1).reset_index(drop=True)\n",
    "    print('\\033[1m' + \"Rebalanced: \"+ '\\033[0m')\n",
    "    check_balance_reads(predData)\n",
    "    \n",
    "    return classData, predData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(classData, predData):\n",
    "    #Set role for each model\n",
    "    class_y = classData['Helpfulness_Label']\n",
    "    pred_y = predData['Total_Reads']\n",
    "    \n",
    "    #Drop target attribute\n",
    "    class_X = classData.drop(['Helpfulness_Label'], axis = 1)\n",
    "    pred_X = predData.drop(['Total_Reads'], axis = 1)\n",
    "    \n",
    "    #Normalize\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    class_X = MinMaxScaler().fit_transform(class_X)\n",
    "    pred_X = MinMaxScaler().fit_transform(pred_X)\n",
    "    \n",
    "    #Split data for each model\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    class_X_train, class_X_test, class_y_train, class_y_test = train_test_split(class_X, class_y, test_size = 0.2)\n",
    "    pred_X_train, pred_X_test, pred_y_train, pred_y_test = train_test_split(pred_X, pred_y, test_size = 0.2)\n",
    "\n",
    "    return class_X_train, class_X_test, class_y_train, class_y_test, pred_X_train, pred_X_test, pred_y_train, pred_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print results of classification models\n",
    "def show_results(title, data, prediction):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import itertools\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    #Print title of model\n",
    "    print('\\033[34m' + '\\033[1m' + '\\033[4m' + title + '\\033[0m')\n",
    "    \n",
    "    #Get confusion matrix\n",
    "    matrix = confusion_matrix(data, prediction)\n",
    "\n",
    "    #Print confusion matrix\n",
    "    print('Confusion matrix')\n",
    "    print(confusion_matrix(data, prediction))\n",
    "    \n",
    "    #Setup sensitivity and specificity\n",
    "    total=sum(sum(matrix))\n",
    "\n",
    "    accuracy=(matrix[0,0]+matrix[1,1])/total\n",
    "    print ('Accuracy : ', round(accuracy, 2), '%')\n",
    "\n",
    "    sensitivity = matrix[0,0]/(matrix[0,0]+matrix[0,1])\n",
    "    print('Sensitivity : ', round(sensitivity, 2), '%')\n",
    "\n",
    "    specificity = matrix[1,1]/(matrix[1,0]+matrix[1,1])\n",
    "    print('Specificity : ', round(specificity, 2), '%')\n",
    "    print('')\n",
    "    print('Classification Report:')\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    print(classification_report(data, prediction, zero_division=0))\n",
    "\n",
    "    \n",
    "#Function to setup data for ROC curves\n",
    "def setup_roc(data, model, probability):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    #Allocate ROC values\n",
    "    fpr, tpr, threshold = roc_curve(data, model.predict_proba(probability)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "\n",
    "def get_cluster_classification(title, class_X_train, class_y_train, class_X_test, class_y_test):\n",
    "    ##Setup Models\n",
    "    #Initialise models \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    NN = MLPClassifier(hidden_layer_sizes=(25, 25, 25), max_iter=10000, shuffle=True)\n",
    "    DT=DecisionTreeClassifier(random_state=42)\n",
    "    RF = RandomForestClassifier(n_estimators= 100, random_state=42)\n",
    "    \n",
    "    #Fit models\n",
    "    NN = NN.fit(class_X_train, class_y_train)\n",
    "    DT=DT.fit(class_X_train,class_y_train)\n",
    "    RF.fit(class_X_train, class_y_train)\n",
    "\n",
    "    #Get predictions\n",
    "    NN_predict = NN.predict(class_X_test)   \n",
    "    DT_predict=DT.predict(class_X_test)\n",
    "    RF_predict=RF.predict(class_X_test)\n",
    "  \n",
    "    ##Evaluate data\n",
    "    #Display results\n",
    "    show_results(\"MLP Test of Cluster: %s\" % (title), class_y_test, NN_predict)\n",
    "    show_results(\"DT Test of Cluster: %s\" % (title), class_y_test, DT_predict)\n",
    "    show_results(\"RF Test of Cluster: %s\" % (title), class_y_test, RF_predict)\n",
    "\n",
    "    ##Evaluate Classification Models\n",
    "    #Evaluate with cross-validation\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    NN_scores = cross_val_score(NN, class_X_train, class_y_train, cv=5)\n",
    "    DT_scores = cross_val_score(DT, class_X_train, class_y_train, cv=5)\n",
    "    RF_scores = cross_val_score(RF, class_X_train, class_y_train, cv=5)\n",
    "\n",
    "    print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"Cross-Validation Comparisons for: \" + title + '\\033[0m')\n",
    "    print(\"NN Accuracy: %0.2f (+/- %0.2f)\" % (NN_scores.mean(), NN_scores.std() * 2))\n",
    "    print(\"DT Accuracy: %0.2f (+/- %0.2f)\" % (DT_scores.mean(), DT_scores.std() * 2))\n",
    "    print(\"RF Accuracy: %0.2f (+/- %0.2f)\" % (RF_scores.mean(), RF_scores.std() * 2))\n",
    "\n",
    "    #Evaluate with ROC curves\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fpr_mean_NN, tpr_mean_NN, roc_auc_mean_NN = 0,0,0\n",
    "    fpr_mean_DT, tpr_mean_DT, roc_auc_mean_DT = 0,0,0\n",
    "    fpr_mean_RF, tpr_mean_RF, roc_auc_mean_RF = 0,0,0\n",
    "\n",
    "    iterations = 5\n",
    "    for r in range(0, iterations):\n",
    "        ##Setup each model\n",
    "        #MLP Test\n",
    "        fpr1, tpr1, roc_auc1 = setup_roc(class_y_test, NN, class_X_test)\n",
    "        fpr_mean_NN += fpr1\n",
    "        tpr_mean_NN += tpr1 \n",
    "        roc_auc_mean_NN += roc_auc1\n",
    "\n",
    "        #DT Test\n",
    "        fpr2, tpr2, roc_auc2 = setup_roc(class_y_test, DT, class_X_test)\n",
    "        fpr_mean_DT += fpr2\n",
    "        tpr_mean_DT += tpr2\n",
    "        roc_auc_mean_DT += roc_auc2\n",
    "\n",
    "        #RF Test\n",
    "        fpr3, tpr3, roc_auc3 = setup_roc(class_y_test, RF, class_X_test)\n",
    "        fpr_mean_RF += fpr3\n",
    "        tpr_mean_RF += tpr3 \n",
    "        roc_auc_mean_RF += roc_auc3\n",
    "    \n",
    "    #Average all values\n",
    "    fpr_mean_NN/=iterations\n",
    "    tpr_mean_NN/=iterations\n",
    "    roc_auc_mean_NN/=iterations \n",
    "    fpr_mean_DT/=iterations \n",
    "    tpr_mean_DT/=iterations\n",
    "    roc_auc_mean_DT/=iterations \n",
    "    fpr_mean_RF/=iterations\n",
    "    tpr_mean_RF/=iterations \n",
    "    roc_auc_mean_RF/=iterations\n",
    "    \n",
    "    #Plot ROC\n",
    "    plt.figure()\n",
    "    plt.title('Helpfulness Label Classfication ROC Curves of: %s' % (title))\n",
    "    plt.plot(fpr_mean_NN, tpr_mean_NN, label = 'NN Test AUC = %0.2f' % roc_auc_mean_NN)\n",
    "    plt.plot(fpr_mean_DT, tpr_mean_DT, label = 'DT Test AUC = %0.2f' % roc_auc_mean_DT)\n",
    "    plt.plot(fpr_mean_RF, tpr_mean_RF, label = 'RF Test AUC = %0.2f' % roc_auc_mean_RF)\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Base AUC = 0.5')\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.xlabel('Specificity')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_prediction(title, pred_X_train, pred_y_train, pred_X_test, pred_y_test):\n",
    "    ##Prediction Model (Total_Reads)\n",
    "    #Set up prediciton \n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    pred = MLPRegressor(hidden_layer_sizes=(25, 25, 25), max_iter=10000)\n",
    "    RF_pred = RandomForestRegressor(n_estimators = 100, random_state = 1)\n",
    "    \n",
    "    #Fit model\n",
    "    pred.fit(pred_X_train, pred_y_train)\n",
    "    \n",
    "    #Get prediction\n",
    "    RF_pred.fit(pred_X_train, pred_y_train)\n",
    "    \n",
    "    rmse_validation = 0\n",
    "    rmse_val_RF = 0\n",
    "    prediction_validation = 0\n",
    "    RF_pred_val= 0\n",
    "    iterations = 100\n",
    "    for k in range(0, iterations):\n",
    "        #Get prediction\n",
    "        prediction=pred.predict(pred_X_test)\n",
    "        RF_prediction = RF_pred.predict(pred_X_test)\n",
    "        #Filter for negative reads to 0\n",
    "        for num, value in enumerate(prediction):\n",
    "            prediction[num] = max(0, value)\n",
    "            prediction[num] = round(prediction[num])\n",
    "            \n",
    "        for num, value in enumerate(RF_prediction):\n",
    "            RF_prediction[num] = max(0, value)\n",
    "            RF_prediction[num] = round(RF_prediction[num])\n",
    "            \n",
    "        prediction_validation += prediction\n",
    "        RF_pred_val += RF_prediction\n",
    "\n",
    "        #Get RMSE\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        RF_mse= mean_squared_error(pred_y_test, RF_prediction)\n",
    "        mse= mean_squared_error(pred_y_test, prediction)\n",
    "        from math import sqrt\n",
    "        rmse_validation += sqrt(mse)\n",
    "        rmse_val_RF += sqrt(RF_mse)\n",
    "\n",
    "    #Average values\n",
    "    prediction_validation/=iterations\n",
    "    RF_pred_val/=iterations\n",
    "    \n",
    "    #Print RMSE values\n",
    "    print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"Mean's of RMSE & r^2 after %d iterations of: %s\" \\\n",
    "          % (iterations, title) + '\\033[0m')\n",
    "    print('Mean NN RMSE:', rmse_validation/iterations)\n",
    "    print('Mean RF RMSE:', rmse_val_RF/iterations)\n",
    "    \n",
    "    ##Initialise residual plots\n",
    "    #NN Residual\n",
    "    import yellowbrick\n",
    "    from yellowbrick.regressor import ResidualsPlot\n",
    "    visualizer = ResidualsPlot(pred, hist=False)\n",
    "    \n",
    "    #Fit data\n",
    "    visualizer.fit(pred_X_train, pred_y_train)  # Fit the training data to the visualizer\n",
    "    visualizer.score(pred_X_test, pred_y_test)  # Evaluate the model on the test data\n",
    "    visualizer.show()                 # Finalize and render the figure\n",
    "    \n",
    "    #Repeat for RF\n",
    "    visualizer_RF = ResidualsPlot(RF_pred, hist=False)\n",
    "\n",
    "    visualizer_RF.fit(pred_X_train, pred_y_train)  # Fit the training data to the visualizer\n",
    "    visualizer_RF.score(pred_X_test, pred_y_test)  # Evaluate the model on the test data\n",
    "    visualizer_RF.show()                 # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve balanced cluster dataframes\n",
    "classData_cNorm, predData_cNorm = set_balance(inputData, \"No Clusters\")\n",
    "classData_c0, predData_c0 = set_balance(inputData[inputData['cluster_number'] == 0], \"Cluster 0\")\n",
    "classData_c1, predData_c1 = set_balance(inputData[inputData['cluster_number'] == 1], \"Cluster 1\")\n",
    "\n",
    "#Remove cluster label now data is split (lead to inaccuracies )\n",
    "classData_cNorm = classData_cNorm.drop([\"cluster_number\"], axis = 1)\n",
    "predData_cNorm = predData_cNorm.drop([\"cluster_number\"], axis = 1)\n",
    "classData_c0 = classData_c0.drop([\"cluster_number\"], axis = 1)\n",
    "predData_c0 = predData_c0.drop([\"cluster_number\"], axis = 1)\n",
    "classData_c1 = classData_c1.drop([\"cluster_number\"], axis = 1)\n",
    "predData_c1 = predData_c1.drop([\"cluster_number\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve training and testing arrays for each cluster\n",
    "#No Cluster\n",
    "class_X_train_cNorm, class_X_test_cNorm, class_y_train_cNorm, class_y_test_cNorm, pred_X_train_cNorm, pred_X_test_cNorm, \\\n",
    "pred_y_train_cNorm, pred_y_test_cNorm = get_train_test(classData_cNorm, predData_cNorm)\n",
    "\n",
    "#Cluster 0\n",
    "class_X_train_c0, class_X_test_c0, class_y_train_c0, class_y_test_c0, pred_X_train_c0, pred_X_test_c0, \\\n",
    "pred_y_train_c0, pred_y_test_c0 = get_train_test(classData_c0, predData_c0)\n",
    "\n",
    "#CLuster 1\n",
    "class_X_train_c1, class_X_test_c1, class_y_train_c1, class_y_test_c1, pred_X_train_c1, pred_X_test_c1, \\\n",
    "pred_y_train_c1, pred_y_test_c1 = get_train_test(classData_c1, predData_c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Cluster Classification\n",
    "get_cluster_classification(\"No Clusters\", class_X_train_cNorm, class_y_train_cNorm, \\\n",
    "                           class_X_test_cNorm, class_y_test_cNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 0 Classification\n",
    "get_cluster_classification(\"Cluster 0\", class_X_train_c0, class_y_train_c0, class_X_test_c0, class_y_test_c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 1 Classification\n",
    "get_cluster_classification(\"Cluster 1\", class_X_train_c1, class_y_train_c1, class_X_test_c1, class_y_test_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Cluster Prediction\n",
    "get_cluster_prediction(\"No Clusters\", pred_X_train_cNorm, pred_y_train_cNorm, pred_X_test_cNorm, pred_y_test_cNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cluster 0 Prediction\n",
    "get_cluster_prediction(\"Cluster 0\", pred_X_train_c0, pred_y_train_c0, pred_X_test_c0, pred_y_test_c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster 1 Prediction\n",
    "get_cluster_prediction(\"Cluster 1\", pred_X_train_c1, pred_y_train_c1, pred_X_test_c1, pred_y_test_c1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
