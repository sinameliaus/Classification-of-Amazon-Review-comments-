{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get label encoder\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "useData=pd.read_excel('AdvancedBDAnalytics_2020_S1_A2_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to check the balance of Helpfulness_Label\n",
    "#Returns number and percentage of each Label\n",
    "def check_help_balance(dataframe):\n",
    "    print('Number of HELPFUL: ', len(dataframe[dataframe['Helpfulness_Label'] == 'HELPFUL']))\n",
    "    print('Number of UNHELPFUL: ', len(dataframe[dataframe['Helpfulness_Label'] == 'UNHELPFUL']))\n",
    "    print('The ratio of the dataset is (HELPFUL / UNHELPFUL): %.2f%% / %.2f%%' % \n",
    "          ((round((len(dataframe[dataframe['Helpfulness_Label'] == 'HELPFUL']) / len(dataframe)) * 100 , 2)), \n",
    "          (round((len(dataframe[dataframe['Helpfulness_Label'] == 'UNHELPFUL']) / len(dataframe)) * 100 , 2))))\n",
    "    print(\"\")\n",
    "\n",
    "##Function to check the balance of Total_Reads\n",
    "#Returns number and percentage of each Label\n",
    "def check_read_balance(dataframe):\n",
    "    print('Number of 0 reads: ', len(dataframe[dataframe['Total_Reads'] == 0]))\n",
    "    print('Number of greater-than 0 reads: ', len(dataframe[dataframe['Total_Reads'] > 0]))\n",
    "    print('The ratio of the dataset is (zero / greater-than zero): %.2f%% / %.2f%%' % \n",
    "          ((round((len(dataframe[dataframe['Total_Reads'] == 0]) / len(dataframe)) * 100 , 2)), \n",
    "          (round((len(dataframe[dataframe['Total_Reads'] > 0]) / len(dataframe)) * 100 , 2))))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Exploration\n",
    "#Check Dimensions of data\n",
    "print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"The shape of the inital dataset is (Rows, Columns):\\n\" + '\\033[0m' \\\n",
    "      + str(useData.shape))\n",
    "print(\"\")\n",
    "\n",
    "#Check data types\n",
    "print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"The data types of the inital dataset are:\\n\" + '\\033[0m' + \\\n",
    "     str(useData.dtypes))\n",
    "print(\"\")\n",
    "\n",
    "#Check for Null Values\n",
    "null_data = useData[useData.columns[useData.isnull().any()]].isnull().sum()\n",
    "print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"The columns and number of rows with null values in dataset:\\n\" + '\\033[0m' + \\\n",
    "     str(list(useData[useData.columns[useData.isnull().any()]].columns.values)) + '\\n' + str(list(null_data)))\n",
    "\n",
    "#Check for duplicates\n",
    "data_duplicates = useData[useData.duplicated('Review_Text') == True]\n",
    "print('\\033[34m' + '\\033[1m' + '\\033[4m' + \"\\nThe number of duplicate Review_Texts in dataset:\\n\" + '\\033[0m' + \\\n",
    "     str(data_duplicates.shape[0]))\n",
    "\n",
    "#Check the balance of Class Targert\n",
    "print('\\033[34m' + '\\033[1m' + '\\033[4m' + '\\nBalance of classification target (Helpfulness_Label)' + '\\033[0m')\n",
    "check_help_balance(useData)\n",
    "\n",
    "#Check the balance of Prediction Targert\n",
    "print('\\033[34m' + '\\033[1m' + '\\033[4m' + '\\nBalance of prediction target (Total_Reads)' + '\\033[0m')\n",
    "check_read_balance(useData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore noise in target label for prediction (Total_Reads)\n",
    "import matplotlib.pyplot as plt\n",
    "print('\\033[34m' + '\\033[1m' + '\\033[4m' + '\\nNoise of prediction target (Total_Reads)' + '\\033[0m')\n",
    "\n",
    "plt.plot(range(0, len(useData)), useData['Total_Reads'], linewidth=2, linestyle=\"-\", c=\"b\")\n",
    "plt.plot([0,len(useData)], [10,10], c=\"r\")\n",
    "plt.xlabel('Row Number')\n",
    "plt.ylabel('Total Reads')\n",
    "\n",
    "plt.show() \n",
    "plt.close()\n",
    "print('\\033[1m' + 'Shape of Total_Reads >10 (Rows, Columns):' + '\\033[0m' + \\\n",
    "      str(useData[useData['Total_Reads'] > 10].shape))\n",
    "print('\\033[1m' + 'Shape of Total_Reads <=10 (Rows, Columns):' + '\\033[0m' + \\\n",
    "      str(useData[useData['Total_Reads'] <= 10].shape))\n",
    "\n",
    "#Initialise Table\n",
    "fig = plt.figure(dpi=80)\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "#Setup Table_Data\n",
    "table_data=[[\"Percentage of Data\", \"Highest Value in Percentage\"]]\n",
    "noise_data = useData['Total_Reads']\n",
    "percents = 0.9\n",
    "\n",
    "#Add points\n",
    "while percents <= 1:\n",
    "    table_data.append([round(percents,2), round(noise_data.quantile(percents),2)])\n",
    "    percents += 0.01\n",
    "\n",
    "#Plot table \n",
    "table = ax.table(cellText=table_data, loc='center')\n",
    "table.set_fontsize(14)\n",
    "table.scale(1,4)\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View correlation of balanced dataset\n",
    "sns.heatmap(useData.corr(), annot=True, fmt='.1g', vmin=-1, vmax=1, center= 0, cmap='Blues', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data preperation\n",
    "#Remove columns that are not required\n",
    "useData = useData.drop([\"Review_ID\", \"Product_ID\", \"User_ID\"], axis = 1)\n",
    "\n",
    "#Remove Null Entries\n",
    "useData = useData.dropna()\n",
    "\n",
    "#Drop duplicates in dataset\n",
    "useData.drop_duplicates('Review_Text', keep='last', inplace=True)\n",
    "\n",
    "#Transform Product_Category and Helpfulness_Label to numeric values (0=HELPFUL, 1=UNHELPFUL)\n",
    "useData[['Helpfulness_Label']] = useData[['Helpfulness_Label']].apply(le.fit_transform)\n",
    "useData[['Product_Category']] = useData[['Product_Category']].apply(le.fit_transform)\n",
    "\n",
    "#Remove noise\n",
    "useData = useData[useData['Total_Reads'] <= 4]\n",
    "\n",
    "#Shuffle dataframe and reset indexs to match number of rows\n",
    "useData = useData.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setup Sentimental Analysis\n",
    "sentence_list = useData['Review_Text'].tolist()\n",
    "summary_list = useData['Review_Summary'].tolist()\n",
    "sentimental_text_list = []\n",
    "sentimental_summary_list = []\n",
    "\n",
    "#Import and initialise VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Append sentimental list's with compunded sentimental score\n",
    "for sentence in sentence_list:\n",
    "    text_score = analyser.polarity_scores(sentence)['compound']\n",
    "    sentimental_text_list.append(text_score)\n",
    "\n",
    "for summary in summary_list:\n",
    "    summary_score = analyser.polarity_scores(summary)['compound']\n",
    "    sentimental_summary_list.append(summary_score)\n",
    "    \n",
    "#Convert to dataframe \n",
    "sentimental_summary_df=pd.DataFrame(sentimental_summary_list,columns=['Sentimental_Summary_Score'])\n",
    "sentimental_text_df=pd.DataFrame(sentimental_text_list,columns=['Sentimental_Text_Score'])\n",
    "\n",
    "#Concatenate to main dataframe\n",
    "sentimentalData = pd.concat([useData, sentimental_summary_df, sentimental_text_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentalData[['Review_Summary','Sentimental_Summary_Score','Review_Text','Sentimental_Text_Score']].iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Setup TF_IDF & Topic generation\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "stop_words = stopwords.words('english')\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "##Bag of words\n",
    "#Tokenize sentence on each row\n",
    "tokenized_text=sentimentalData['Review_Text'].apply(word_tokenize)\n",
    "tokenized_summary=sentimentalData['Review_Summary'].apply(word_tokenize)\n",
    "\n",
    "#Convert tokens to lowercase\n",
    "lower_text=tokenized_text.apply(lambda x: [word.lower() for word in x])\n",
    "topic_text=tokenized_text.apply(lambda x: [word.lower() for word in x])\n",
    "lower_summary=tokenized_summary.apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "#Remove any stop words\n",
    "filtered_text=lower_text.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "filtered_summary=lower_summary.apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Implement stemming\n",
    "filtered_stemized_text=filtered_text.apply(lambda x: [snowball_stemmer.stem(word) for word in x ])\n",
    "topic_stemized_text=topic_text.apply(lambda x: [snowball_stemmer.stem(word) for word in x ])\n",
    "filtered_stemized_summary=filtered_summary.apply(lambda x: [snowball_stemmer.stem(word) for word in x ])\n",
    "\n",
    "\n",
    "#Remove words less than 4 chars or more than 11\n",
    "filtered_length_text=filtered_stemized_text.apply(lambda x: [word for word in x if len(word)>3 and\n",
    "len(word)<12])\n",
    "topic_length_text=topic_stemized_text.apply(lambda x: [word for word in x if len(word)>3 and\n",
    "len(word)<12])\n",
    "filtered_length_summary=filtered_stemized_summary.apply(lambda x: [word for word in x if len(word)>3 and\n",
    "len(word)<12])\n",
    "\n",
    "##Convert to token counts\n",
    "#Initialise count vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(lowercase=True, preprocessor = lambda x: x, tokenizer = lambda\n",
    "x: x)\n",
    "\n",
    "#Create term doc matrices\n",
    "text_vectorized=vectorizer.fit_transform(filtered_length_text).toarray()\n",
    "summary_vectorized=vectorizer.fit_transform(filtered_length_summary).toarray()\n",
    "topic_vectorized=vectorizer.fit_transform(topic_length_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Find TF_IDF\n",
    "#Initialise TF-TF_IDF converter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "\n",
    "#Perform TF_IDF on term doc matrices\n",
    "TFIDF_text = tfidfconverter.fit_transform(text_vectorized).toarray()\n",
    "TFIDF_summary = tfidfconverter.fit_transform(summary_vectorized).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TFIDF_text.shape)\n",
    "print(TFIDF_summary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Implement SVD\n",
    "#Create titles for each SVD column\n",
    "n_components = 3\n",
    "i=1\n",
    "column_names = []\n",
    "while i <= n_components:\n",
    "    cName = 'SVD Text' + str(i)\n",
    "    column_names.append(cName)\n",
    "    i += 1\n",
    "    \n",
    "i=1\n",
    "summary_column_names = []\n",
    "while i <= n_components:\n",
    "    cName = 'SVD Summary' + str(i)\n",
    "    summary_column_names.append(cName)\n",
    "    i += 1\n",
    "\n",
    "#Initialise SVD model\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=n_components, algorithm='randomized', n_iter=100)\n",
    "\n",
    "#Perform dimension reduction\n",
    "svd_text_vectorized=svd_model.fit_transform(TFIDF_text)\n",
    "svd_summary_vectorized=svd_model.fit_transform(TFIDF_summary)\n",
    "\n",
    "#Convert array to a dataframe\n",
    "svd_text_df = pd.DataFrame(svd_text_vectorized, columns=column_names)\n",
    "svd_summary_df = pd.DataFrame(svd_summary_vectorized, columns=summary_column_names)\n",
    "\n",
    "#Add SVD to main dataframe\n",
    "data_SVD= pd.concat([sentimentalData, svd_text_df, svd_summary_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Topic Modelling\n",
    "#Initialise LDA model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "LDA_text = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "##Fit term doc matrices into LDA\n",
    "LDA_text.fit(topic_vectorized)\n",
    "\n",
    "#Retrieve topic percentages from each row\n",
    "text_topic_values = LDA_text.transform(topic_vectorized)\n",
    "\n",
    "#Create new column and add topic number with highest percentage\n",
    "data_topics = data_SVD\n",
    "data_topics['Text_Topic'] = text_topic_values.argmax(axis=1)\n",
    "\n",
    "for i,topic in enumerate(LDA_text.components_):\n",
    "    print('Top 10 words for Text Topic #%s:' % (i+1))\n",
    "    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_clusters(cMethod, data):\n",
    "    ##Evaluate how many clusters to use\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    #Remove Review Summary and Review_Text (No longer required)\n",
    "    data_clustered = data.drop([\"Review_Summary\", \"Review_Text\"], axis = 1)\n",
    "\n",
    "    #Initialise cluster validation df\n",
    "    cluster_val = pd.DataFrame(columns = [\"Davies Bouldin Score\", \"Silhouette Score\", \"Calinski Harabaz Score\"])\n",
    "\n",
    "    #Loop through 2-10 clusters\n",
    "    for k in range(2, 11):\n",
    "        #Initialise clusters with n_clusters=loop value\n",
    "        clust = cMethod(n_clusters=k, max_iter=10000)\n",
    "\n",
    "        #Fit model and assign labels to an array\n",
    "        My_clustering=clust.fit(data_clustered)\n",
    "        labels = My_clustering.labels_\n",
    "\n",
    "        #Ignore warnings due to Davies Bouldin Score producing annoying runtime warning\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        #Setup score gathering metrics\n",
    "        from sklearn.metrics import davies_bouldin_score\n",
    "        from sklearn.metrics import silhouette_score\n",
    "        from sklearn import metrics\n",
    "        from sklearn.metrics import pairwise_distances\n",
    "\n",
    "        #Set each score into array having 1 tuple for the three scores\n",
    "        scores = [(round(davies_bouldin_score(data_clustered, labels), 3), \n",
    "                        round(silhouette_score(data_clustered, labels), 3),  \n",
    "                        round(metrics.calinski_harabaz_score(data_clustered, labels), 3))]\n",
    "\n",
    "        #Initilise and fill iteration cluster df with the scores\n",
    "        cluster_iter = pd.DataFrame(scores, columns = [\"Davies Bouldin Score\", \"Silhouette Score\", \"Calinski Harabaz Score\"])\n",
    "\n",
    "        #Append to main df\n",
    "        cluster_val = cluster_val.append(cluster_iter, ignore_index=True, sort = False)\n",
    "\n",
    "    #Return warnings to normal\n",
    "    warnings.filterwarnings(\"default\")\n",
    "\n",
    "    #Change indexs and index title to match clusters\n",
    "    cluster_val = cluster_val.set_index([pd.Index([2,3,4,5,6,7,8,9,10])])\n",
    "    cluster_val.index.names = ['Clusters']\n",
    "    print(cluster_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decide how many clusters to use\n",
    "from sklearn.cluster import KMeans\n",
    "compare_clusters(KMeans, data_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise cluster analysis\n",
    "from sklearn.cluster import KMeans\n",
    "chosen_clusters = 2\n",
    "My_kmeans = KMeans(n_clusters=chosen_clusters, max_iter=10000)\n",
    "    \n",
    "#Remove Review Summary and Review_Text (No longer required and cannot be used in clustering)\n",
    "data_clustered = data_topics.drop([\"Review_Summary\", \"Review_Text\"], axis = 1)\n",
    "\n",
    "#Fit model\n",
    "My_clustering=My_kmeans.fit(data_clustered)\n",
    "labels = My_clustering.labels_\n",
    "\n",
    "#Set labels column\n",
    "data_clustered['cluster_number']=My_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plot clusters\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Create blobs\n",
    "data = make_blobs(n_samples=500, n_features=2, centers=2, cluster_std=1, random_state=1)\n",
    "points = data[0]\n",
    "y_km = My_clustering.fit_predict(points)\n",
    "centres = My_clustering.cluster_centers_\n",
    "\n",
    "#Plot Cluster 0 (Red)\n",
    "plt.scatter(points[y_km ==0,0], points[y_km == 0,1], s=100, c='r')\n",
    "plotLabel2 = plt.annotate('c0', centres[0], horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',c='w')\n",
    "\n",
    "#Plot Cluster 1(Black)\n",
    "plt.scatter(points[y_km ==1,0], points[y_km == 1,1], s=100, c='k')\n",
    "plotLabel1 = plt.annotate('c1', centres[1], horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=20, weight='bold',c='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send prepared data to excel\n",
    "data_clustered.to_excel(\"BOYCE_Data_Preperation.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
